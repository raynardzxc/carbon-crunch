Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
## add filenames here as we submit
filenames <- c("27875.csv",
"ensemble_xgb1_rf.csv",
"28114.csv",
"30004_x.csv",
"30384.csv",
"31175_x.csv",
"31746.csv",
"31840.csv",
"32977.csv",
"37238.csv")
k <- length(filenames)
result_matrix <- matrix(NA, nrow = k, ncol = k)
for (i in 1:k){
benchmark <- read.csv(filenames[i])
benchmark <- subset(benchmark, select = -c(No))
for (n in 1:k){
submission <- read.csv(filenames[n])
submission <- subset(submission, select = -c(No))
## applying the function row-wise to the dataframe
benchmark_new <- benchmark
benchmark_new$Choice <- apply(benchmark, 1, get_choice)
result_matrix[i,n] <- logloss(benchmark_new, submission)
}
}
## presenting data
result_df <- as.data.frame(result_matrix)
colnames(result_df) <- filenames
result_df$Benchmark <- filenames
result_df <- result_df[c(c("Benchmark"),filenames)]
result_df
rm(list=ls())
get_choice <- function(row_data) {
max_val <- max(row_data)
choice <- which(row_data == max_val)
return(choice[1])
}
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
## add filenames here as we submit
filenames <- c("27875.csv",
"ensemble_xgb1_rf.csv",
"28114.csv",
#"30004_x.csv",
"30384.csv",
#"31175_x.csv",
"31746.csv",
"31840.csv",
"32977.csv",
"37238.csv")
k <- length(filenames)
result_matrix <- matrix(NA, nrow = k, ncol = k)
for (i in 1:k){
benchmark <- read.csv(filenames[i])
benchmark <- subset(benchmark, select = -c(No))
for (n in 1:k){
submission <- read.csv(filenames[n])
submission <- subset(submission, select = -c(No))
## applying the function row-wise to the dataframe
benchmark_new <- benchmark
benchmark_new$Choice <- apply(benchmark, 1, get_choice)
result_matrix[i,n] <- logloss(benchmark_new, submission)
}
}
## presenting data
result_df <- as.data.frame(result_matrix)
colnames(result_df) <- filenames
result_df$Benchmark <- filenames
result_df <- result_df[c(c("Benchmark"),filenames)]
result_df
heatmap(result_matrix, Rowv = NA, Colv = NA, col = heat.colors(256), scale = "none")
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]
tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
max_depth = c(5),
eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
# Train the XGBoost classifer
xgb.fit <- train(x = train.data,
y = train.label,
method = "xgbTree",
trControl = ctrl,
tuneGrid = grid)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]
tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
max_depth = c(5),
eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree
ctrl <- trainControl(method = "cv", number = 5)
# Train the XGBoost classifer
xgb.fit <- train(x = train.data,
y = train.label,
method = "xgbTree",
trControl = ctrl,
tuneGrid = grid)
# Review the final model and results
xgb.fit
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]
tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
max_depth = c(5),
eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
num_class = length(levels(choices))
params = list(
eta=0.1, ## lower implies larger nrounds, means more robust to overfitting but slower to compute
max_depth=5,
gamma=0, ## pruning var, larger more conservative
subsample=0.8,
colsample_bytree=1,
min_child_weight=3,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
xgb.newfit=xgb.train(
params=params,
data=xgb.train,
nrounds=150,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
xgb.pred = predict(xgb.newfit,test.data,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(choices)
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)
num_class = length(levels(choices))
params = list(
eta=0.1, ## lower implies larger nrounds, means more robust to overfitting but slower to compute
max_depth=5,
gamma=0, ## pruning var, larger more conservative
subsample=0.8,
colsample_bytree=1,
min_child_weight=3,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
xgb.newfit=xgb.train(
params=params,
data=xgb.train,
nrounds=150,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
xgb.pred = predict(xgb.newfit,xgb.test,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(choices)
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]
tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
max_depth = c(5),
eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
num_class = length(levels(choices))
params = list(
eta=0.1, ## lower implies larger nrounds, means more robust to overfitting but slower to compute
max_depth=5,
gamma=0, ## pruning var, larger more conservative
subsample=0.8,
colsample_bytree=1,
min_child_weight=3,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
xgb.newfit=xgb.train(
params=params,
data=xgb.train,
nrounds=150,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
xgb.pred = predict(xgb.newfit,xgb.test,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(choices)
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("dplyr","xgboost","splitTools", "caret")
# Use our custom load function
loadPkgs(pkgnames)
totrain <- read.csv("train1_onehot2.csv")
pure <- totrain
topredict <- read.csv("test1_onehot2.csv")
# Convert the Choices factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
totrain$Choice <- as.factor(totrain$Choice)
choices <- totrain$Choice
label <- as.integer(totrain$Choice)-1
totrain$Choice <- NULL
totrain <- subset(totrain, select = -c(Case, No))
totrain <- subset(totrain, select = -c(CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
set.seed(123)
inds <- createDataPartition(pure$Choice, p = 0.8, list = FALSE)
train.data = as.matrix(totrain[inds, ])
train.label = label[inds]
test.data = as.matrix(totrain[-inds, ])
test.label = label[-inds]
tempno <- pure$No[-inds]
tempchoice <- pure$Choice[-inds]
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)
grid <- expand.grid(nrounds = c(50, 100, 150), ## times the algo add a new decision tree to the ensemble model, higher more complex
max_depth = c(5),
eta = c(0.01, 0.1, 0.3), ## steps taken when updating the weights, higher faster
gamma = c(0, 2, 4), ## minimum loss reduction required to further partition, higher more simple
colsample_bytree = c(0.3, 0.5, 0.7, 0.9, 1), ## what fraction of features are used to train each tree, higher means all
min_child_weight = c(1, 3, 5), ## decides if new child will be better, higher more simple
subsample = c(0.6, 0.7, 0.8)) ## what fraction of train data are used to train each tree
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE)
num_class = length(levels(choices))
params = list(
eta=0.1, ## lower implies larger nrounds, means more robust to overfitting but slower to compute
max_depth=5,
gamma=0, ## pruning var, larger more conservative
subsample=0.8,
colsample_bytree=1,
min_child_weight=3,
objective="multi:softprob",
eval_metric="mlogloss",
num_class=num_class
)
xgb.newfit=xgb.train(
params=params,
data=xgb.train,
nrounds=150,
watchlist=list(val1=xgb.train,val2=xgb.test),
verbose=0
)
xgb.pred = predict(xgb.newfit,xgb.test,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(choices)
## add back 1 for the testing data's Choice
test.label <- test.label+1
test.label <- as.data.frame(test.label)
colnames(test.label) <- c("Choice")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
colnames(xgb.pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss(test.label, xgb.pred)
shiny::runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
# initialize state values
day <- reactiveVal(1)
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
test_df <- data.frame(level = 1:3,
cash_generated = c(10, 15, 30),
emissions = c(10, 15, 20),
solar_consumption = c(2, 5, 10),
cost = c(0, 15, 30),
linetype = c(0,1,0,1,0,1))
View(test_df)
test_df <- data.frame(level = c(1,1,2,2,3,3),
cash_generated = c(10, 14, 15, 25, 30, 40),
emissions = c(10, 13, 15, 18, 20, 23),
solar_consumption = c(2, 4, 5, 7, 10, 12),
cost = c(0, 0, 15, 20, 30, 40),
linetype = c(0,1,0,1,0,1))
View(test_df)
test_df$cash_generated[1]
View(test_df)
?where
test_df$cash_generated[test_df$level==1 & test_df$linetype==0]
test_df$cash_generated[test_df$level==1 & test_df$linetype==1]
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
runApp('GitHub/carbon-crunch/carboncrunch')
shiny::runApp()
runApp()
